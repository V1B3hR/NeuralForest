name: NauraLas

on:
  push: 
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read

env:
  PYTORCH_VERSION: "2.1.0"
  CUDA_VERSION: "12.1"

jobs:
  train-cpu:
    name: Train on CPU
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: read
      actions: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies (CPU)
        run: |
          python -m pip install --upgrade pip
          pip install torch==${{ env.PYTORCH_VERSION }} --index-url https://download.pytorch.org/whl/cpu
          pip install -r requirements.txt
      
      - name: Verify PyTorch installation
        run: |
          python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
      
      - name: Run CPU training
        run: |
          python ecosystem_simulation.py --epochs 10 --batch-size 16
      
      - name: Upload training results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: training-results-cpu
          path: |
            results/
            **/*.png
            **/*.log
          retention-days: 7

  train-gpu:
    name: Train on Single GPU (T4)
    runs-on: ubuntu-latest-4-cores-16gb-gpu-t4
    timeout-minutes: 120
    permissions:
      contents: read
      actions: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install CUDA Toolkit
        uses: Jimver/cuda-toolkit@v0.2.15
        with:
          cuda: '12.1.0'
      
      - name: Install dependencies (GPU)
        run: |
          python -m pip install --upgrade pip
          pip install torch==${{ env.PYTORCH_VERSION }} --index-url https://download.pytorch.org/whl/cu121
          pip install -r requirements.txt
      
      - name: Verify GPU availability
        run: |
          nvidia-smi
          python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
          python -c "import torch; print(f'CUDA version: {torch.version.cuda}')"
          python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
          python -c "import torch; print(f'GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
      
      - name: Run GPU training with mixed precision
        run: |
          python ecosystem_simulation.py --epochs 100 --batch-size 128 --use-amp --device cuda
      
      - name: Run phase6 demo on GPU
        run: |
          python phase6_demo.py --device cuda
      
      - name: GPU Benchmark
        run: |
          python -c "
          import torch
          import time
          
          if torch.cuda.is_available():
              device = torch.device('cuda')
              print('Running GPU benchmark...')
              
              # Matrix multiplication benchmark
              size = 4096
              iterations = 100
              
              a = torch.randn(size, size, device=device)
              b = torch.randn(size, size, device=device)
              
              # Warmup
              for _ in range(10):
                  c = torch.matmul(a, b)
              torch.cuda.synchronize()
              
              # Benchmark
              start = time.time()
              for _ in range(iterations):
                  c = torch.matmul(a, b)
              torch.cuda.synchronize()
              end = time.time()
              
              elapsed = end - start
              ops_per_sec = iterations / elapsed
              
              print(f'Matrix size: {size}x{size}')
              print(f'Iterations: {iterations}')
              print(f'Total time: {elapsed:.2f}s')
              print(f'Operations per second: {ops_per_sec:.2f}')
              print(f'Time per operation: {elapsed/iterations*1000:.2f}ms')
          else:
              print('GPU not available')
          "
      
      - name: GPU Memory Report
        run: |
          python -c "
          import torch
          
          if torch.cuda.is_available():
              print('GPU Memory Report:')
              for i in range(torch.cuda.device_count()):
                  props = torch.cuda.get_device_properties(i)
                  print(f'\nGPU {i}: {props.name}')
                  print(f'  Total memory: {props.total_memory / 1024**3:.2f} GB')
                  print(f'  Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB')
                  print(f'  Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB')
          "
      
      - name: Upload training results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: training-results-gpu
          path: |
            results/
            **/*.png
            **/*.log
          retention-days: 14
      
      - name: Upload TensorBoard logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: tensorboard-logs
          path: runs/
          retention-days: 14

  train-gpu-multi:
    name: Train on Multi-GPU (A100)
    runs-on: ubuntu-latest-8-cores-32gb-gpu-a100
    timeout-minutes: 180
    if: github.event_name == 'workflow_dispatch'
    permissions:
      contents: read
      actions: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install CUDA Toolkit
        uses: Jimver/cuda-toolkit@v0.2.15
        with:
          cuda: '12.1.0'
      
      - name: Install dependencies (GPU)
        run: |
          python -m pip install --upgrade pip
          pip install torch==${{ env.PYTORCH_VERSION }} --index-url https://download.pytorch.org/whl/cu121
          pip install -r requirements.txt
      
      - name: Verify Multi-GPU availability
        run: |
          nvidia-smi
          python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
          python -c "
          import torch
          if torch.cuda.is_available():
              for i in range(torch.cuda.device_count()):
                  print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
          "
      
      - name: Run Multi-GPU training
        run: |
          torchrun --nproc_per_node=auto --nnodes=1 ecosystem_simulation.py --epochs 200 --batch-size 256 --distributed
      
      - name: Upload training results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: training-results-multi-gpu
          path: |
            results/
            **/*.png
            **/*.log
          retention-days: 30

  train-self-hosted-gpu:
    name: Train on Self-Hosted GPU
    runs-on: self-hosted-gpu
    timeout-minutes: 180
    if: github.event_name == 'workflow_dispatch'
    permissions:
      contents: read
      actions: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python environment
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Verify GPU availability
        run: |
          nvidia-smi || echo "nvidia-smi not available"
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
          python -c "import torch; print(f'GPU count: {torch.cuda.device_count() if torch.cuda.is_available() else 0}')"
      
      - name: Run long training
        run: |
          python ecosystem_simulation.py --epochs 500 --batch-size 256 --use-amp --gradient-checkpointing --device cuda
      
      - name: Upload training results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: training-results-self-hosted
          path: |
            results/
            **/*.png
            **/*.log
          retention-days: 90
